{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import DatabaseManager\n",
    "\n",
    "db=DatabaseManager()\n",
    "df=db.execute_query('select * from marketing_campaign',fetch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>year_birth</th>\n",
       "      <th>education</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>income</th>\n",
       "      <th>kidhome</th>\n",
       "      <th>teenhome</th>\n",
       "      <th>dt_customer</th>\n",
       "      <th>recency</th>\n",
       "      <th>mntwines</th>\n",
       "      <th>...</th>\n",
       "      <th>numwebvisitsmonth</th>\n",
       "      <th>acceptedcmp3</th>\n",
       "      <th>acceptedcmp4</th>\n",
       "      <th>acceptedcmp5</th>\n",
       "      <th>acceptedcmp1</th>\n",
       "      <th>acceptedcmp2</th>\n",
       "      <th>complain</th>\n",
       "      <th>z_costcontact</th>\n",
       "      <th>z_revenue</th>\n",
       "      <th>response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5524</td>\n",
       "      <td>1957</td>\n",
       "      <td>Graduation</td>\n",
       "      <td>Single</td>\n",
       "      <td>58138.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2012-09-04</td>\n",
       "      <td>58</td>\n",
       "      <td>635</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2174</td>\n",
       "      <td>1954</td>\n",
       "      <td>Graduation</td>\n",
       "      <td>Single</td>\n",
       "      <td>46344.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2014-03-08</td>\n",
       "      <td>38</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4141</td>\n",
       "      <td>1965</td>\n",
       "      <td>Graduation</td>\n",
       "      <td>Together</td>\n",
       "      <td>71613.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2013-08-21</td>\n",
       "      <td>26</td>\n",
       "      <td>426</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6182</td>\n",
       "      <td>1984</td>\n",
       "      <td>Graduation</td>\n",
       "      <td>Together</td>\n",
       "      <td>26646.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-02-10</td>\n",
       "      <td>26</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5324</td>\n",
       "      <td>1981</td>\n",
       "      <td>PhD</td>\n",
       "      <td>Married</td>\n",
       "      <td>58293.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-01-19</td>\n",
       "      <td>94</td>\n",
       "      <td>173</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     id  year_birth   education marital_status   income  kidhome  teenhome  \\\n",
       "0  5524        1957  Graduation         Single  58138.0        0         0   \n",
       "1  2174        1954  Graduation         Single  46344.0        1         1   \n",
       "2  4141        1965  Graduation       Together  71613.0        0         0   \n",
       "3  6182        1984  Graduation       Together  26646.0        1         0   \n",
       "4  5324        1981         PhD        Married  58293.0        1         0   \n",
       "\n",
       "  dt_customer  recency  mntwines  ...  numwebvisitsmonth  acceptedcmp3  \\\n",
       "0  2012-09-04       58       635  ...                  7             0   \n",
       "1  2014-03-08       38        11  ...                  5             0   \n",
       "2  2013-08-21       26       426  ...                  4             0   \n",
       "3  2014-02-10       26        11  ...                  6             0   \n",
       "4  2014-01-19       94       173  ...                  5             0   \n",
       "\n",
       "   acceptedcmp4  acceptedcmp5  acceptedcmp1  acceptedcmp2  complain  \\\n",
       "0             0             0             0             0         0   \n",
       "1             0             0             0             0         0   \n",
       "2             0             0             0             0         0   \n",
       "3             0             0             0             0         0   \n",
       "4             0             0             0             0         0   \n",
       "\n",
       "   z_costcontact  z_revenue  response  \n",
       "0              3         11         1  \n",
       "1              3         11         0  \n",
       "2              3         11         0  \n",
       "3              3         11         0  \n",
       "4              3         11         0  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'year_birth', 'income', 'kidhome', 'teenhome', 'recency',\n",
       "       'mntwines', 'mntfruits', 'mntmeatproducts', 'mntfishproducts',\n",
       "       'mntsweetproducts', 'mntgoldprods', 'numdealspurchases',\n",
       "       'numwebpurchases', 'numcatalogpurchases', 'numstorepurchases',\n",
       "       'numwebvisitsmonth', 'acceptedcmp3', 'acceptedcmp4', 'acceptedcmp5',\n",
       "       'acceptedcmp1', 'acceptedcmp2', 'complain', 'z_costcontact',\n",
       "       'z_revenue', 'response'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_col=df.columns[df.dtypes=='object']\n",
    "df.columns[df.dtypes!='object']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "CustomException",
     "evalue": "Error occured in python script name [C:\\Users\\shukl\\AppData\\Local\\Temp\\ipykernel_7332\\831343499.py] line number [156] error message [unsupported operand type(s) for -: 'str' and 'str']",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Python_project\\Customer-Personality-Analysis\\venv\\lib\\site-packages\\pandas\\core\\ops\\array_ops.py:171\u001b[0m, in \u001b[0;36m_na_arithmetic_op\u001b[1;34m(left, right, op, is_cmp)\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 171\u001b[0m     result \u001b[39m=\u001b[39m func(left, right)\n\u001b[0;32m    172\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Python_project\\Customer-Personality-Analysis\\venv\\lib\\site-packages\\pandas\\core\\roperator.py:15\u001b[0m, in \u001b[0;36mrsub\u001b[1;34m(left, right)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrsub\u001b[39m(left, right):\n\u001b[1;32m---> 15\u001b[0m     \u001b[39mreturn\u001b[39;00m right \u001b[39m-\u001b[39;49m left\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'str' and 'str'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 156\u001b[0m, in \u001b[0;36mDataTransformation.initiate_data_transformation\u001b[1;34m(self, filename)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[39m# Apply the full pipeline on the training dataset\u001b[39;00m\n\u001b[1;32m--> 156\u001b[0m input_feature_train_arr \u001b[39m=\u001b[39m full_pipeline\u001b[39m.\u001b[39;49mfit_transform(df_train)\n\u001b[0;32m    158\u001b[0m \u001b[39m# Save the full pipeline object\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python_project\\Customer-Personality-Analysis\\venv\\lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1146\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Python_project\\Customer-Personality-Analysis\\venv\\lib\\site-packages\\sklearn\\pipeline.py:464\u001b[0m, in \u001b[0;36mPipeline.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    463\u001b[0m fit_params_steps \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_fit_params(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[1;32m--> 464\u001b[0m Xt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fit(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params_steps)\n\u001b[0;32m    466\u001b[0m last_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_final_estimator\n",
      "File \u001b[1;32mc:\\Python_project\\Customer-Personality-Analysis\\venv\\lib\\site-packages\\sklearn\\pipeline.py:370\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[1;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[0;32m    369\u001b[0m \u001b[39m# Fit or load from cache the current transformer\u001b[39;00m\n\u001b[1;32m--> 370\u001b[0m X, fitted_transformer \u001b[39m=\u001b[39m fit_transform_one_cached(\n\u001b[0;32m    371\u001b[0m     cloned_transformer,\n\u001b[0;32m    372\u001b[0m     X,\n\u001b[0;32m    373\u001b[0m     y,\n\u001b[0;32m    374\u001b[0m     \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    375\u001b[0m     message_clsname\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPipeline\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    376\u001b[0m     message\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_log_message(step_idx),\n\u001b[0;32m    377\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params_steps[name],\n\u001b[0;32m    378\u001b[0m )\n\u001b[0;32m    379\u001b[0m \u001b[39m# Replace the transformer of the step with the fitted\u001b[39;00m\n\u001b[0;32m    380\u001b[0m \u001b[39m# transformer. This is necessary when loading the transformer\u001b[39;00m\n\u001b[0;32m    381\u001b[0m \u001b[39m# from the cache.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python_project\\Customer-Personality-Analysis\\venv\\lib\\site-packages\\joblib\\memory.py:353\u001b[0m, in \u001b[0;36mNotMemorizedFunc.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    352\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 353\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunc(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Python_project\\Customer-Personality-Analysis\\venv\\lib\\site-packages\\sklearn\\pipeline.py:950\u001b[0m, in \u001b[0;36m_fit_transform_one\u001b[1;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[0;32m    949\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(transformer, \u001b[39m\"\u001b[39m\u001b[39mfit_transform\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 950\u001b[0m     res \u001b[39m=\u001b[39m transformer\u001b[39m.\u001b[39mfit_transform(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[0;32m    951\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Python_project\\Customer-Personality-Analysis\\venv\\lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1146\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Python_project\\Customer-Personality-Analysis\\venv\\lib\\site-packages\\sklearn\\pipeline.py:464\u001b[0m, in \u001b[0;36mPipeline.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    463\u001b[0m fit_params_steps \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_fit_params(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[1;32m--> 464\u001b[0m Xt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fit(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params_steps)\n\u001b[0;32m    466\u001b[0m last_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_final_estimator\n",
      "File \u001b[1;32mc:\\Python_project\\Customer-Personality-Analysis\\venv\\lib\\site-packages\\sklearn\\pipeline.py:370\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[1;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[0;32m    369\u001b[0m \u001b[39m# Fit or load from cache the current transformer\u001b[39;00m\n\u001b[1;32m--> 370\u001b[0m X, fitted_transformer \u001b[39m=\u001b[39m fit_transform_one_cached(\n\u001b[0;32m    371\u001b[0m     cloned_transformer,\n\u001b[0;32m    372\u001b[0m     X,\n\u001b[0;32m    373\u001b[0m     y,\n\u001b[0;32m    374\u001b[0m     \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    375\u001b[0m     message_clsname\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPipeline\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    376\u001b[0m     message\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_log_message(step_idx),\n\u001b[0;32m    377\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params_steps[name],\n\u001b[0;32m    378\u001b[0m )\n\u001b[0;32m    379\u001b[0m \u001b[39m# Replace the transformer of the step with the fitted\u001b[39;00m\n\u001b[0;32m    380\u001b[0m \u001b[39m# transformer. This is necessary when loading the transformer\u001b[39;00m\n\u001b[0;32m    381\u001b[0m \u001b[39m# from the cache.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python_project\\Customer-Personality-Analysis\\venv\\lib\\site-packages\\joblib\\memory.py:353\u001b[0m, in \u001b[0;36mNotMemorizedFunc.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    352\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 353\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunc(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Python_project\\Customer-Personality-Analysis\\venv\\lib\\site-packages\\sklearn\\pipeline.py:950\u001b[0m, in \u001b[0;36m_fit_transform_one\u001b[1;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[0;32m    949\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(transformer, \u001b[39m\"\u001b[39m\u001b[39mfit_transform\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 950\u001b[0m     res \u001b[39m=\u001b[39m transformer\u001b[39m.\u001b[39mfit_transform(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[0;32m    951\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Python_project\\Customer-Personality-Analysis\\venv\\lib\\site-packages\\sklearn\\utils\\_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[0;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 140\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    141\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m    142\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python_project\\Customer-Personality-Analysis\\venv\\lib\\site-packages\\sklearn\\base.py:915\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    914\u001b[0m     \u001b[39m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[1;32m--> 915\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit(X, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\u001b[39m.\u001b[39;49mtransform(X)\n\u001b[0;32m    916\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    917\u001b[0m     \u001b[39m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python_project\\Customer-Personality-Analysis\\venv\\lib\\site-packages\\sklearn\\utils\\_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[0;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 140\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    141\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m    142\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python_project\\Customer-Personality-Analysis\\venv\\lib\\site-packages\\sklearn\\preprocessing\\_function_transformer.py:240\u001b[0m, in \u001b[0;36mFunctionTransformer.transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    239\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_input(X, reset\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m--> 240\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_transform(X, func\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunc, kw_args\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkw_args)\n",
      "File \u001b[1;32mc:\\Python_project\\Customer-Personality-Analysis\\venv\\lib\\site-packages\\sklearn\\preprocessing\\_function_transformer.py:312\u001b[0m, in \u001b[0;36mFunctionTransformer._transform\u001b[1;34m(self, X, func, kw_args)\u001b[0m\n\u001b[0;32m    310\u001b[0m     func \u001b[39m=\u001b[39m _identity\n\u001b[1;32m--> 312\u001b[0m \u001b[39mreturn\u001b[39;00m func(X, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m(kw_args \u001b[39mif\u001b[39;00m kw_args \u001b[39melse\u001b[39;00m {}))\n",
      "Cell \u001b[1;32mIn[5], line 33\u001b[0m, in \u001b[0;36mDataTransformation.calculate_customer_for\u001b[1;34m(self, df)\u001b[0m\n\u001b[0;32m     32\u001b[0m newest_customer_date \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39mdt_customer\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mmax()\n\u001b[1;32m---> 33\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mCustomer_For\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m (newest_customer_date \u001b[39m-\u001b[39;49m df[\u001b[39m'\u001b[39;49m\u001b[39mdt_customer\u001b[39;49m\u001b[39m'\u001b[39;49m])\u001b[39m.\u001b[39mdt\u001b[39m.\u001b[39mdays\n\u001b[0;32m     34\u001b[0m \u001b[39mreturn\u001b[39;00m df\n",
      "File \u001b[1;32mc:\\Python_project\\Customer-Personality-Analysis\\venv\\lib\\site-packages\\pandas\\core\\ops\\common.py:81\u001b[0m, in \u001b[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m     79\u001b[0m other \u001b[39m=\u001b[39m item_from_zerodim(other)\n\u001b[1;32m---> 81\u001b[0m \u001b[39mreturn\u001b[39;00m method(\u001b[39mself\u001b[39;49m, other)\n",
      "File \u001b[1;32mc:\\Python_project\\Customer-Personality-Analysis\\venv\\lib\\site-packages\\pandas\\core\\arraylike.py:198\u001b[0m, in \u001b[0;36mOpsMixin.__rsub__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    196\u001b[0m \u001b[39m@unpack_zerodim_and_defer\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m__rsub__\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    197\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__rsub__\u001b[39m(\u001b[39mself\u001b[39m, other):\n\u001b[1;32m--> 198\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_arith_method(other, roperator\u001b[39m.\u001b[39;49mrsub)\n",
      "File \u001b[1;32mc:\\Python_project\\Customer-Personality-Analysis\\venv\\lib\\site-packages\\pandas\\core\\series.py:6112\u001b[0m, in \u001b[0;36mSeries._arith_method\u001b[1;34m(self, other, op)\u001b[0m\n\u001b[0;32m   6111\u001b[0m \u001b[39mself\u001b[39m, other \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39malign_method_SERIES(\u001b[39mself\u001b[39m, other)\n\u001b[1;32m-> 6112\u001b[0m \u001b[39mreturn\u001b[39;00m base\u001b[39m.\u001b[39;49mIndexOpsMixin\u001b[39m.\u001b[39;49m_arith_method(\u001b[39mself\u001b[39;49m, other, op)\n",
      "File \u001b[1;32mc:\\Python_project\\Customer-Personality-Analysis\\venv\\lib\\site-packages\\pandas\\core\\base.py:1348\u001b[0m, in \u001b[0;36mIndexOpsMixin._arith_method\u001b[1;34m(self, other, op)\u001b[0m\n\u001b[0;32m   1347\u001b[0m \u001b[39mwith\u001b[39;00m np\u001b[39m.\u001b[39merrstate(\u001b[39mall\u001b[39m\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m-> 1348\u001b[0m     result \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39;49marithmetic_op(lvalues, rvalues, op)\n\u001b[0;32m   1350\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_construct_result(result, name\u001b[39m=\u001b[39mres_name)\n",
      "File \u001b[1;32mc:\\Python_project\\Customer-Personality-Analysis\\venv\\lib\\site-packages\\pandas\\core\\ops\\array_ops.py:232\u001b[0m, in \u001b[0;36marithmetic_op\u001b[1;34m(left, right, op)\u001b[0m\n\u001b[0;32m    230\u001b[0m     \u001b[39m# error: Argument 1 to \"_na_arithmetic_op\" has incompatible type\u001b[39;00m\n\u001b[0;32m    231\u001b[0m     \u001b[39m# \"Union[ExtensionArray, ndarray[Any, Any]]\"; expected \"ndarray[Any, Any]\"\u001b[39;00m\n\u001b[1;32m--> 232\u001b[0m     res_values \u001b[39m=\u001b[39m _na_arithmetic_op(left, right, op)  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    234\u001b[0m \u001b[39mreturn\u001b[39;00m res_values\n",
      "File \u001b[1;32mc:\\Python_project\\Customer-Personality-Analysis\\venv\\lib\\site-packages\\pandas\\core\\ops\\array_ops.py:178\u001b[0m, in \u001b[0;36m_na_arithmetic_op\u001b[1;34m(left, right, op, is_cmp)\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_cmp \u001b[39mand\u001b[39;00m (is_object_dtype(left\u001b[39m.\u001b[39mdtype) \u001b[39mor\u001b[39;00m is_object_dtype(right)):\n\u001b[0;32m    174\u001b[0m     \u001b[39m# For object dtype, fallback to a masked operation (only operating\u001b[39;00m\n\u001b[0;32m    175\u001b[0m     \u001b[39m#  on the non-missing values)\u001b[39;00m\n\u001b[0;32m    176\u001b[0m     \u001b[39m# Don't do this for comparisons, as that will handle complex numbers\u001b[39;00m\n\u001b[0;32m    177\u001b[0m     \u001b[39m#  incorrectly, see GH#32047\u001b[39;00m\n\u001b[1;32m--> 178\u001b[0m     result \u001b[39m=\u001b[39m _masked_arith_op(left, right, op)\n\u001b[0;32m    179\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Python_project\\Customer-Personality-Analysis\\venv\\lib\\site-packages\\pandas\\core\\ops\\array_ops.py:135\u001b[0m, in \u001b[0;36m_masked_arith_op\u001b[1;34m(x, y, op)\u001b[0m\n\u001b[0;32m    134\u001b[0m     \u001b[39mif\u001b[39;00m mask\u001b[39m.\u001b[39many():\n\u001b[1;32m--> 135\u001b[0m         result[mask] \u001b[39m=\u001b[39m op(xrav[mask], y)\n\u001b[0;32m    137\u001b[0m np\u001b[39m.\u001b[39mputmask(result, \u001b[39m~\u001b[39mmask, np\u001b[39m.\u001b[39mnan)\n",
      "File \u001b[1;32mc:\\Python_project\\Customer-Personality-Analysis\\venv\\lib\\site-packages\\pandas\\core\\roperator.py:15\u001b[0m, in \u001b[0;36mrsub\u001b[1;34m(left, right)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrsub\u001b[39m(left, right):\n\u001b[1;32m---> 15\u001b[0m     \u001b[39mreturn\u001b[39;00m right \u001b[39m-\u001b[39;49m left\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'str' and 'str'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mCustomException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 180\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[39m# Note: The code above includes both the unchanged portions and the applied changes based on the provided suggestions. \u001b[39;00m\n\u001b[0;32m    178\u001b[0m \u001b[39m# However, the actual implementation might need to include the missing imports, definitions, and context from your complete codebase.\u001b[39;00m\n\u001b[0;32m    179\u001b[0m dt\u001b[39m=\u001b[39mDataTransformation()\n\u001b[1;32m--> 180\u001b[0m a\u001b[39m=\u001b[39mdt\u001b[39m.\u001b[39;49minitiate_data_transformation(\u001b[39m'\u001b[39;49m\u001b[39mmarketing_campaign\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m    182\u001b[0m a\n",
      "Cell \u001b[1;32mIn[5], line 173\u001b[0m, in \u001b[0;36mDataTransformation.initiate_data_transformation\u001b[1;34m(self, filename)\u001b[0m\n\u001b[0;32m    171\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    172\u001b[0m     logging\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mError occurred during reading database table \u001b[39m\u001b[39m{\u001b[39;00mfilename\u001b[39m}\u001b[39;00m\u001b[39m in data transformation error: \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m--> 173\u001b[0m     \u001b[39mraise\u001b[39;00m CustomException(e,sys)\n\u001b[0;32m    174\u001b[0m     \u001b[39m# Optionally, you can rollback the transaction here if necessary\u001b[39;00m\n\u001b[0;32m    175\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdb\u001b[39m.\u001b[39mrollback_transaction()\n",
      "\u001b[1;31mCustomException\u001b[0m: Error occured in python script name [C:\\Users\\shukl\\AppData\\Local\\Temp\\ipykernel_7332\\831343499.py] line number [156] error message [unsupported operand type(s) for -: 'str' and 'str']"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dataclasses import dataclass\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from src.exception import CustomException\n",
    "from src.logger import logging\n",
    "from src.utils import DatabaseManager, save_object\n",
    "from datetime import datetime\n",
    "import sys\n",
    "\n",
    "# Define the list of categorical columns\n",
    "object_cols = ['education', 'living_with', 'is_parent']\n",
    "\n",
    "@dataclass\n",
    "class DataTransformationConfig:\n",
    "    preprocessor_obj_file_path: str = os.path.join('artifacts', 'preprocessor.pkl')\n",
    "\n",
    "class DataTransformation:\n",
    "    def __init__(self):\n",
    "        self.data_transformation_config = DataTransformationConfig()\n",
    "        self.db = DatabaseManager()\n",
    "        self.LE = LabelEncoder()  # Instantiate LabelEncoder\n",
    "\n",
    "    def calculate_customer_for(self, df):\n",
    "        newest_customer_date = df['dt_customer'].max()\n",
    "        df['Customer_For'] = (newest_customer_date - df['dt_customer']).dt.days\n",
    "        return df\n",
    "\n",
    "    def calculate_age(self, df):\n",
    "        current_year = datetime.now().year\n",
    "        df['Age'] = current_year - df['year_birth']\n",
    "        return df\n",
    "\n",
    "    def calculate_spent(self, df):\n",
    "        df['Spent'] = df[['mntwines', 'mntfruits', 'mntmeatproducts', 'mntfishproducts', 'mntsweetproducts', 'mntgoldprods']].sum(axis=1)\n",
    "        return df\n",
    "\n",
    "    def preprocess_living_with(self, df):\n",
    "        df['living_with'] = df['marital_status'].replace({\"Married\": \"Partner\", \"Together\": \"Partner\", \"Absurd\": \"Alone\", \"Widow\": \"Alone\", \"YOLO\": \"Alone\", \"Divorced\": \"Alone\", \"Single\": \"Alone\"})\n",
    "        return df\n",
    "\n",
    "    def calculate_family_size(self, df):\n",
    "        df['Family_Size'] = df['living_with'].replace({\"Alone\": 1, \"Partner\": 2})\n",
    "        return df\n",
    "\n",
    "    def calculate_is_parent(self, df):\n",
    "        df['Is_Parent'] = np.where(df['children'] > 0, 1, 0)\n",
    "        return df\n",
    "\n",
    "    def preprocess_education(self, df):\n",
    "        df['education'] = df['education'].replace({\"Basic\": \"Undergraduate\", \"2n Cycle\": \"Undergraduate\", \"Graduation\": \"Graduate\", \"Master\": \"Postgraduate\", \"PhD\": \"Postgraduate\"})\n",
    "        return df\n",
    "\n",
    "    def drop_columns(self, df):\n",
    "        to_drop = [\"marital_status\", \"dt_customer\", \"z_costcontact\", \"z_revenue\", \"year_birth\", \"id\"]\n",
    "        df.drop(to_drop, axis=1, inplace=True)\n",
    "        return df\n",
    "\n",
    "    def transform_categorical(self, df):\n",
    "        # Use the predefined object_cols list for transformation\n",
    "        s = (df.dtypes == 'object')\n",
    "        object_cols = list(s[s].index)\n",
    "        for col in object_cols:\n",
    "            df[col] = self.LE.fit_transform(df[col])\n",
    "        return df\n",
    "\n",
    "    def get_data_preprocessing_pipeline(self) -> Pipeline:\n",
    "        try:\n",
    "            # Create a function transformer for each preprocessing step\n",
    "            customer_for_transformer = FunctionTransformer(self.calculate_customer_for)\n",
    "            age_transformer = FunctionTransformer(self.calculate_age)\n",
    "            spent_transformer = FunctionTransformer(self.calculate_spent)\n",
    "            living_with_transformer = FunctionTransformer(self.preprocess_living_with)\n",
    "            family_size_transformer = FunctionTransformer(self.calculate_family_size)\n",
    "            is_parent_transformer = FunctionTransformer(self.calculate_is_parent)\n",
    "            education_transformer = FunctionTransformer(self.preprocess_education)\n",
    "            drop_columns_transformer = FunctionTransformer(self.drop_columns)\n",
    "            transform_categorical_transformer = FunctionTransformer(self.transform_categorical)\n",
    "\n",
    "            # Create the data preprocessing pipeline\n",
    "            data_preprocessing_pipeline = Pipeline([\n",
    "                ('customer_for', customer_for_transformer),\n",
    "                ('age', age_transformer),\n",
    "                ('spent', spent_transformer),\n",
    "                ('living_with', living_with_transformer),\n",
    "                ('family_size', family_size_transformer),\n",
    "                ('is_parent', is_parent_transformer),\n",
    "                ('education', education_transformer),\n",
    "                ('drop_columns', drop_columns_transformer),\n",
    "                ('transform_categorical', transform_categorical_transformer),\n",
    "                ('imputer_num', SimpleImputer(strategy='median')),  # Imputer for numerical columns\n",
    "                ('imputer_cat', SimpleImputer(strategy='most_frequent')),  # Imputer for categorical columns\n",
    "                ('scaler', StandardScaler()),  # StandardScaler for both types of columns\n",
    "            ])\n",
    "\n",
    "            return data_preprocessing_pipeline\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.info(\"Error in Data Preprocessing Pipeline\")\n",
    "            raise CustomException(e,sys)\n",
    "\n",
    "    def get_full_pipeline(self, numerical_cols, categorical_cols):\n",
    "        try:\n",
    "            # Get the data preprocessing pipeline object\n",
    "            data_preprocessing_pipeline = self.get_data_preprocessing_pipeline()\n",
    "\n",
    "            # Create the combined pipeline\n",
    "            full_pipeline = Pipeline([\n",
    "                ('data_preprocessing', data_preprocessing_pipeline),\n",
    "                ('pca', PCA(n_components=3))  # PCA for numerical columns\n",
    "            ])\n",
    "\n",
    "            return full_pipeline\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.info(\"Error in Full Pipeline\")\n",
    "            raise CustomException(e,sys)\n",
    "\n",
    "    def initiate_data_transformation(self, filename):\n",
    "        try:\n",
    "            logging.info(f'Reading database table {filename} initiated')\n",
    "\n",
    "            df = self.db.execute_query(f'select * from {filename}', fetch=True)\n",
    "\n",
    "            # Define feature columns\n",
    "            categorical_cols =['education', 'marital_status', 'dt_customer']\n",
    "            numerical_cols = ['id', 'year_birth', 'income', 'kidhome', 'teenhome', 'recency',\n",
    "            'mntwines', 'mntfruits', 'mntmeatproducts', 'mntfishproducts',\n",
    "            'mntsweetproducts', 'mntgoldprods', 'numdealspurchases',\n",
    "            'numwebpurchases', 'numcatalogpurchases', 'numstorepurchases',\n",
    "            'numwebvisitsmonth', 'acceptedcmp3', 'acceptedcmp4', 'acceptedcmp5',\n",
    "            'acceptedcmp1', 'acceptedcmp2', 'complain', 'z_costcontact',\n",
    "            'z_revenue', 'response']\n",
    "\n",
    "            logging.info(f'Reading database table {filename} complete')\n",
    "            logging.info(f'Dataframe Head: \\n{df.head().to_string()}')\n",
    "\n",
    "            logging.info('Obtaining preprocessing object')\n",
    "\n",
    "            # Get the full pipeline object\n",
    "            full_pipeline = self.get_full_pipeline(numerical_cols, categorical_cols)\n",
    "\n",
    "            # Drop the specified columns from the DataFrame\n",
    "            cols_del = ['acceptedcmp3', 'acceptedcmp4', 'acceptedcmp5', 'acceptedcmp1',\n",
    "                        'acceptedcmp2', 'complain', 'response']\n",
    "            df_train = df.drop(columns=cols_del, axis=1)\n",
    "\n",
    "            # Apply the full pipeline on the training dataset\n",
    "            input_feature_train_arr = full_pipeline.fit_transform(df_train)\n",
    "\n",
    "            # Save the full pipeline object\n",
    "            save_object(\n",
    "                file_path=self.data_transformation_config.preprocessor_obj_file_path,\n",
    "                obj=full_pipeline\n",
    "            )\n",
    "\n",
    "            logging.info('Full pipeline pickle file saved')\n",
    "\n",
    "            return (\n",
    "                input_feature_train_arr,\n",
    "                self.data_transformation_config.preprocessor_obj_file_path\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.info(f'Error occurred during reading database table {filename} in data transformation error: {e}')\n",
    "            raise CustomException(e,sys)\n",
    "            # Optionally, you can rollback the transaction here if necessary\n",
    "            self.db.rollback_transaction()\n",
    "\n",
    "# Note: The code above includes both the unchanged portions and the applied changes based on the provided suggestions. \n",
    "# However, the actual implementation might need to include the missing imports, definitions, and context from your complete codebase.\n",
    "dt=DataTransformation()\n",
    "a=dt.initiate_data_transformation('marketing_campaign')\n",
    "\n",
    "a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
